#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ã‚’ä½¿ã£ã¦PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã—ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ã¨æ¯”è¼ƒã™ã‚‹ãƒ„ãƒ¼ãƒ«

ä½¿ç”¨æ–¹æ³•:
python lambda_csv_comparison.py --limit 10
"""

import pandas as pd
import json
import os
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
from difflib import SequenceMatcher
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
import time

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®rootãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
os.environ.setdefault("GOOGLE_CLOUD_REGION", "us-central1")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "./credential.json")
os.environ.setdefault("GEMINI_MODEL", "gemini-2.0-flash")

from src.ocr import extract_text
from src.llm import extract_information
from src.processor import process_extracted_data
from src.formatter import format_sqs_message
from src.utils.helper import convert_bounding_box_format

def read_csv_data(csv_path: str) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    return pd.read_csv(csv_path)

def process_pdf_file(pdf_path: str) -> Dict[str, Any]:
    """æŒ‡å®šã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†ã‚’å®Ÿè¡Œ"""
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")
    
    try:
        # OCRå‡¦ç†
        ocr_response = extract_text(str(pdf_path))
        if not ocr_response:
            raise Exception("OCRå‡¦ç†ã§ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # Bounding Boxå½¢å¼ã®å¤‰æ›
        converted_ocr_data = convert_bounding_box_format(ocr_response)
        if not converted_ocr_data:
            raise Exception("Bounding Boxå½¢å¼ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # LLMå‡¦ç†
        file_identifier = os.path.splitext(os.path.basename(pdf_path))[0]  # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤å»
        extracted_info = extract_information(converted_ocr_data, file_identifier)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’åˆ†é›¢
        usage_metadata = extracted_info.pop('usage_metadata', None)
        
        # ãƒ‡ãƒ¼ã‚¿ã®å¾Œå‡¦ç†
        processed_data = process_extracted_data(
            extracted_info,
            ocr_response,
            clipping_request_id=f"test-{os.path.basename(pdf_path)}",
            s3_key=f"test/{os.path.basename(pdf_path)}"
        )
        
        # SQSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_message = format_sqs_message(
            processed_data,
            f"test-{os.path.basename(pdf_path)}",
            f"test/{os.path.basename(pdf_path)}"
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’çµ±ä¸€
        result = {
            'statusCode': 200,
            'body': json.dumps({
                'extracted_data': extracted_info,
                'processed_data': processed_data,
                'formatted_message': formatted_message,
                'usage': usage_metadata or {}
            }, ensure_ascii=False)
        }
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }

def clean_value(value):
    """å€¤ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆNaNã€ç©ºæ–‡å­—åˆ—ãªã©ã‚’Noneã«å¤‰æ›ï¼‰"""
    if pd.isna(value) or value == '' or value == 'nan':
        return None
    if isinstance(value, str):
        return value.strip()
    return value

def normalize_phone_number(phone: str) -> str:
    """é›»è©±ç•ªå·ã‚’æ­£è¦åŒ–ï¼ˆæ•°å­—ã®ã¿ã«å¤‰æ›ã—ã€å…ˆé ­0ã‚’å‰Šé™¤ã—ã¦10æ¡ã¾ãŸã¯9æ¡ã«çµ±ä¸€ï¼‰"""
    if not isinstance(phone, str):
        phone_str = str(phone) if phone is not None else ""
    else:
        phone_str = phone
    
    # æ•°å­—ä»¥å¤–ã‚’é™¤å»
    import re
    normalized = re.sub(r'[^\d]', '', phone_str)
    
    # CSVã§.0ä»˜ãã®å ´åˆã¯æœ«å°¾ã®0ã‚’å‰Šé™¤
    if '.' in phone_str and normalized.endswith('0'):
        normalized = normalized[:-1]
    
    # 11æ¡ã§å…ˆé ­ãŒ0ã®å ´åˆã¯å…ˆé ­ã®0ã‚’å‰Šé™¤ï¼ˆä¾‹ï¼š0474512831 â†’ 474512831ï¼‰
    if len(normalized) == 11 and normalized.startswith('0'):
        normalized = normalized[1:]
    # 10æ¡ã§å…ˆé ­ãŒ0ã®å ´åˆã‚‚å…ˆé ­ã®0ã‚’å‰Šé™¤ï¼ˆä¾‹ï¼š0962938881 â†’ 962938881ï¼‰
    elif len(normalized) == 10 and normalized.startswith('0'):
        normalized = normalized[1:]
    
    return normalized

def normalize_string_for_comparison(text: str) -> str:
    """æ¯”è¼ƒç”¨ã«æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã€ç‰¹æ®Šæ–‡å­—ã€åŠè§’å…¨è§’ã®çµ±ä¸€ï¼‰"""
    if not isinstance(text, str):
        return str(text) if text is not None else ""
    
    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›ã€å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
    import unicodedata
    
    normalized = text
    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
    normalized = unicodedata.normalize('NFKC', normalized)
    # ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»
    normalized = normalized.replace(' ', '').replace('ã€€', '')
    # ãƒã‚¤ãƒ•ãƒ³ã‚’çµ±ä¸€
    normalized = normalized.replace('ãƒ¼', '-').replace('â€•', '-').replace('âˆ’', '-')
    # æ‹¬å¼§ã‚’çµ±ä¸€
    normalized = normalized.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    
    return normalized.lower()

def fuzzy_match_strings(str1: str, str2: str, threshold: float = 0.8) -> bool:
    """ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã§æ–‡å­—åˆ—ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    if str1 is None or str2 is None:
        return str1 == str2
    
    # æ­£è¦åŒ–
    norm_str1 = normalize_string_for_comparison(str1)
    norm_str2 = normalize_string_for_comparison(str2)
    
    # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    if norm_str1 == norm_str2:
        return True
    
    # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆçŸ­ã„æ–¹ãŒé•·ã„æ–¹ã«å«ã¾ã‚Œã‚‹ï¼‰
    if len(norm_str1) > 0 and len(norm_str2) > 0:
        if norm_str1 in norm_str2 or norm_str2 in norm_str1:
            return True
    
    # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
    similarity = SequenceMatcher(None, norm_str1, norm_str2).ratio()
    return similarity >= threshold

def extract_value_from_prediction(prediction) -> Any:
    """äºˆæ¸¬å€¤ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆè¾æ›¸å½¢å¼ã®å ´åˆã¯'value'ã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼‰"""
    if prediction is None or prediction == "":
        return None
    
    if isinstance(prediction, str):
        try:
            # JSONæ–‡å­—åˆ—ã®å ´åˆã¯ãƒ‘ãƒ¼ã‚¹
            parsed = json.loads(prediction.replace("'", '"'))
            if isinstance(parsed, dict) and 'value' in parsed:
                return parsed['value']
            return parsed
        except (json.JSONDecodeError, ValueError):
            return prediction
    elif isinstance(prediction, dict) and 'value' in prediction:
        return prediction['value']
    
    return prediction

def get_nested_value(data: Dict[str, Any], key_path: str) -> Any:
    """ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å€¤ã‚’å–å¾—ã™ã‚‹ï¼ˆä¾‹: 'amount_info.total_amount'ï¼‰"""
    keys = key_path.split('.')
    value = data
    try:
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        return value
    except (KeyError, TypeError):
        return None

def get_tax_breakdown_value(data: Dict[str, Any], tax_rate: float, field_name: str) -> Any:
    """tax_breakdowné…åˆ—ã‹ã‚‰æŒ‡å®šã—ãŸç¨ç‡ã®ç‰¹å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã‚’å–å¾—ã™ã‚‹"""
    try:
        amount_info = data.get('amount_info', {})
        tax_breakdown = amount_info.get('tax_breakdown', [])
        
        for item in tax_breakdown:
            if isinstance(item, dict):
                tax_rate_obj = item.get('tax_rate', {})
                if isinstance(tax_rate_obj, dict) and 'value' in tax_rate_obj:
                    item_tax_rate = tax_rate_obj['value']
                    # ç¨ç‡ãŒä¸€è‡´ã™ã‚‹å ´åˆï¼ˆå°æ•°ç‚¹èª¤å·®ã‚’è€ƒæ…®ï¼‰
                    if abs(item_tax_rate - tax_rate) < 1e-9:
                        target_field = item.get(field_name, {})
                        if isinstance(target_field, dict) and 'value' in target_field:
                            return target_field
                        return target_field
        return None
    except (KeyError, TypeError, AttributeError):
        return None

def compare_values(result_body: Dict[str, Any], csv_row: pd.Series) -> Tuple[Dict[str, bool], Dict[str, Any]]:
    """å‡¦ç†çµæœã¨CSVè¡Œã®å€¤ã‚’æ¯”è¼ƒã—ã€æ¯”è¼ƒçµæœã¨äºˆæ¸¬å€¤ã‚’è¿”ã™"""
    
    # extracted_dataã‹ã‚‰å€¤ã‚’å–å¾—
    extracted_data = result_body.get('extracted_data', {})
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆCSVãƒ˜ãƒƒãƒ€ãƒ¼ -> æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åï¼‰
    # æ³¨æ„: ç¨ç‡åˆ¥ã®é‡‘é¡é …ç›®ï¼ˆ10%ç¨è¾¼é‡‘é¡ç­‰ï¼‰ã¯å°‚ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã§å‡¦ç†ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯å®šç¾©ä¸è¦
    field_mapping = {
        'ãƒãƒƒãƒ—å…¥åŠ›çµæœ': 'chip_input_result',
        'é›»è©±ç•ªå·': 'phone_number',
        'ç™ºè¡Œè€…': 'issuer_name',
        'ç™»éŒ²ç•ªå·': 'registrated_number',
        'ç¹°è¶Šé¡ï¼‹å½“æœˆé¡': 'amount_info.total_amount',
        'æ”¯æ‰•æœŸé™': 'due_date',
        'éŠ€è¡Œå': 'bank_details.bank_name',
        'æ”¯åº—å': 'bank_details.branch_name',
        'å£åº§ç¨®åˆ¥': 'bank_details.account_type',
        'å£åº§ç•ªå·': 'bank_details.account_number',
        'å£åº§åç¾©äºº': 'bank_details.account_holder',
        'æŒ¯è¾¼æ‰‹æ•°æ–™è² æ‹…': 'bank_transfer_fee_payer'
    }
    
    # æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å®šç¾©ï¼ˆãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã‚’é©ç”¨ï¼‰
    string_fields = {
        'ãƒãƒƒãƒ—å…¥åŠ›çµæœ', 'é›»è©±ç•ªå·', 'ç™ºè¡Œè€…', 'ç™»éŒ²ç•ªå·', 'æ”¯æ‰•æœŸé™',
        'éŠ€è¡Œå', 'æ”¯åº—å', 'å£åº§ç¨®åˆ¥', 'å£åº§ç•ªå·', 'å£åº§åç¾©äºº', 'æŒ¯è¾¼æ‰‹æ•°æ–™è² æ‹…'
    }
    
    # å‡¦ç†å¯¾è±¡ã®CSVãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å®šç¾©ï¼ˆfield_mappingã«ãªã„ã‚‚ã®ã‚‚å«ã‚ã‚‹ï¼‰
    all_csv_fields = list(field_mapping.keys()) + [
        '10%ç¨è¾¼é‡‘é¡', '10%ç¨æŠœé‡‘é¡', '10%æ¶ˆè²»ç¨',
        '8%ç¨è¾¼é‡‘é¡', '8%ç¨æŠœé‡‘é¡', '8%æ¶ˆè²»ç¨',
        '0%å¯¾è±¡é‡‘é¡', 'æºæ³‰å¾´åç¨é¡'
    ]
    
    comparison_results = {}
    predicted_values = {}
    
    for csv_field in all_csv_fields:
        # field_mappingã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°None
        extract_field = field_mapping.get(csv_field)
        # CSVå€¤ã®å–å¾—ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        csv_value = clean_value(csv_row.get(csv_field))
        
        # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å€¤å–å¾—ï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸã‚­ãƒ¼ã«å¯¾å¿œï¼‰
        raw_extract_value = None
        
        # ç¨ç‡åˆ¥ã®é‡‘é¡é …ç›®ã¯å°‚ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã§å‡¦ç†
        if csv_field in ['10%ç¨è¾¼é‡‘é¡', '10%ç¨æŠœé‡‘é¡', '10%æ¶ˆè²»ç¨']:
            if csv_field == '10%ç¨è¾¼é‡‘é¡':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.1, 'amount_include_tax')
            elif csv_field == '10%ç¨æŠœé‡‘é¡':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.1, 'amount_exclude_tax')
            elif csv_field == '10%æ¶ˆè²»ç¨':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.1, 'amount_consumption_tax')
        elif csv_field in ['8%ç¨è¾¼é‡‘é¡', '8%ç¨æŠœé‡‘é¡', '8%æ¶ˆè²»ç¨']:
            if csv_field == '8%ç¨è¾¼é‡‘é¡':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.08, 'amount_include_tax')
            elif csv_field == '8%ç¨æŠœé‡‘é¡':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.08, 'amount_exclude_tax')
            elif csv_field == '8%æ¶ˆè²»ç¨':
                raw_extract_value = get_tax_breakdown_value(extracted_data, 0.08, 'amount_consumption_tax')
        elif csv_field == '0%å¯¾è±¡é‡‘é¡':
            # 0%ã¯ç¨ç„¡ã—ã®é‡‘é¡ãªã®ã§ã€tax_free_amountã‹ã‚‰å–å¾—
            raw_extract_value = get_nested_value(extracted_data, 'amount_info.tax_free_amount')
        elif csv_field == 'æºæ³‰å¾´åç¨é¡':
            # æºæ³‰å¾´åç¨é¡ã¯amount_withholdingã‹ã‚‰å–å¾—
            raw_extract_value = get_nested_value(extracted_data, 'amount_info.amount_withholding')
        else:
            # é€šå¸¸ã®ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚­ãƒ¼ã®å‡¦ç†
            if extract_field:
                raw_extract_value = get_nested_value(extracted_data, extract_field)
            else:
                raw_extract_value = None
        
        # äºˆæ¸¬å€¤ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’æŠ½å‡º
        extract_value = extract_value_from_prediction(raw_extract_value)
        extract_value = clean_value(extract_value)
        
        # 0ã‚„0.0ã®å€¤ã¯Lambdaã‹ã‚‰è¿”ã•ã‚Œãªã„æƒ³å®šãªã®ã§Noneã¨ã—ã¦æ‰±ã†
        if extract_value == 0 or extract_value == 0.0 or extract_value == "0" or extract_value == "0.0":
            extract_value = None
        
        # äºˆæ¸¬å€¤ã‚’è¨˜éŒ²ï¼ˆ_predåˆ—ç”¨ï¼‰- valueã®ã¿ã‚’ä¿å­˜
        pred_value_for_csv = None
        if raw_extract_value is not None and isinstance(raw_extract_value, dict) and 'value' in raw_extract_value:
            pred_value_for_csv = raw_extract_value['value']
        elif raw_extract_value is not None:
            pred_value_for_csv = raw_extract_value
        
        # 0ã‚„0.0ã®å€¤ã¯Lambdaã‹ã‚‰è¿”ã•ã‚Œãªã„æƒ³å®šãªã®ã§ç©ºæ–‡å­—ã¨ã—ã¦è¨˜éŒ²
        if pred_value_for_csv == 0 or pred_value_for_csv == 0.0 or pred_value_for_csv == "0" or pred_value_for_csv == "0.0":
            predicted_values[f"{csv_field}_pred"] = ""
        else:
            predicted_values[f"{csv_field}_pred"] = pred_value_for_csv if pred_value_for_csv is not None else ""
        
        # æ¯”è¼ƒ
        # ä¸¡æ–¹ãŒNoneã®å ´åˆã¯True
        if csv_value is None and extract_value is None:
            comparison_results[csv_field] = True
        elif csv_value is None or extract_value is None:
            comparison_results[csv_field] = False
        else:
            # é›»è©±ç•ªå·ã¯å°‚ç”¨ã®æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            if csv_field == 'é›»è©±ç•ªå·':
                csv_phone = normalize_phone_number(str(csv_value))
                extract_phone = normalize_phone_number(str(extract_value))
                comparison_results[csv_field] = csv_phone == extract_phone
            # æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã€æ•°å€¤ã¯é€šå¸¸ã®æ¯”è¼ƒ
            elif csv_field in string_fields:
                comparison_results[csv_field] = fuzzy_match_strings(str(csv_value), str(extract_value))
            else:
                # æ•°å€¤ã®å ´åˆã¯æ•°å€¤ã¨ã—ã¦æ¯”è¼ƒ
                try:
                    csv_num = float(csv_value)
                    extract_num = float(extract_value)
                    comparison_results[csv_field] = abs(csv_num - extract_num) < 0.01  # å¾®å°ãªå·®ã¯è¨±å®¹
                except (ValueError, TypeError):
                    # æ•°å€¤å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦å³å¯†æ¯”è¼ƒ
                    comparison_results[csv_field] = str(csv_value) == str(extract_value)
    
    return comparison_results, predicted_values

def calculate_tokens_used(result_body: Dict[str, Any]) -> Dict[str, int]:
    """çµæœã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è©³ç´°ã‚’å–å¾—"""
    usage = result_body.get('usage', {})
    
    # è©³ç´°ãªãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
    token_details = {
        'input_tokens': 0,
        'output_tokens': 0,
        'cached_input_tokens': 0,
        'total_tokens': 0
    }
    
    if 'prompt_token_count' in usage:
        token_details['input_tokens'] = usage['prompt_token_count']
    if 'candidates_token_count' in usage:
        token_details['output_tokens'] = usage['candidates_token_count']
    if 'cached_content_token_count' in usage:
        token_details['cached_input_tokens'] = usage['cached_content_token_count']
    if 'total_token_count' in usage:
        token_details['total_tokens'] = usage['total_token_count']
    elif token_details['input_tokens'] and token_details['output_tokens']:
        token_details['total_tokens'] = token_details['input_tokens'] + token_details['output_tokens'] + token_details['cached_input_tokens']
    
    return token_details

def process_single_row(args_tuple):
    """ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼šå˜ä¸€è¡Œã‚’å‡¦ç†ã™ã‚‹"""
    index, row, verbose = args_tuple
    
    filename = row['ãƒ•ã‚¡ã‚¤ãƒ«å']
    file_path = row['ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹']
    
    # data/é…ä¸‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    pdf_path = os.path.join('data', file_path)
    
    result_data = {
        'index': index,
        'filename': filename,
        'file_path': file_path,
        'status': 'processing',
        'error': None,
        'result_row': None,
        'tokens': 0
    }
    
    if not os.path.exists(pdf_path):
        result_data['status'] = 'error'
        result_data['error'] = f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}"
        result_data['result_row'] = {
            'ãƒ•ã‚¡ã‚¤ãƒ«å': filename,
            'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹': file_path,
            'ã‚¨ãƒ©ãƒ¼': result_data['error'],
            'input_tokens': 0,
            'output_tokens': 0,
            'cached_input_tokens': 0,
            'total_tokens': 0
        }
        return result_data
    
    try:
        # ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†ã‚’å®Ÿè¡Œ
        result = process_pdf_file(pdf_path)
        
        if result.get('statusCode') != 200:
            error_body = json.loads(result['body'])
            raise Exception(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_body.get('error', 'Unknown error')}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã‚’ãƒ‘ãƒ¼ã‚¹
        body = json.loads(result['body'])
        
        # raw_responseãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®è­¦å‘Šï¼ˆã“ã“ã§ã¯è¨˜éŒ²ã®ã¿ï¼‰
        extracted_data = body.get('extracted_data', {})
        if 'raw_response' in extracted_data:
            result_data['warning'] = f"raw_responseãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {filename}"
        
        # æ¯”è¼ƒå®Ÿè¡Œ
        comparison, predicted_values = compare_values(body, row)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        token_details = calculate_tokens_used(body)
        result_data['tokens'] = token_details['total_tokens']
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹ï¼ˆæ¯”è¼ƒçµæœã¨äºˆæ¸¬å€¤ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹ï¼‰
        result_row = {
            'ãƒ•ã‚¡ã‚¤ãƒ«å': filename,
            'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹': file_path,
            'input_tokens': token_details['input_tokens'],
            'output_tokens': token_details['output_tokens'],
            'cached_input_tokens': token_details['cached_input_tokens'],
            'total_tokens': token_details['total_tokens']
        }
        result_row.update(comparison)
        result_row.update(predicted_values)
        
        # ä¸€è‡´ç‡ã‚’è¨ˆç®—
        match_count = sum(comparison.values())
        total_fields = len(comparison)
        match_rate = (match_count / total_fields) * 100 if total_fields > 0 else 0
        
        result_data['status'] = 'success'
        result_data['result_row'] = result_row
        result_data['match_rate'] = match_rate
        result_data['match_count'] = match_count
        result_data['total_fields'] = total_fields
        
        # è©³ç´°è¡¨ç¤ºã®æº–å‚™ï¼ˆverboseãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰
        if verbose:
            result_data['verbose_info'] = []
            field_mapping = {
                'ãƒãƒƒãƒ—å…¥åŠ›çµæœ': 'chip_input_result',
                'é›»è©±ç•ªå·': 'phone_number',
                'ç™ºè¡Œè€…': 'issuer_name',
                'ç™»éŒ²ç•ªå·': 'registrated_number'
            }
            for csv_field, extract_field in field_mapping.items():
                csv_val = clean_value(row.get(csv_field))
                pred_val = extract_value_from_prediction(predicted_values.get(f"{csv_field}_pred"))
                match = comparison.get(csv_field)
                result_data['verbose_info'].append(f"  {csv_field}: {match} (CSV: '{csv_val}' vs äºˆæ¸¬: '{pred_val}')")
        
        return result_data
        
    except Exception as e:
        result_data['status'] = 'error'
        result_data['error'] = str(e)
        result_data['result_row'] = {
            'ãƒ•ã‚¡ã‚¤ãƒ«å': filename,
            'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹': file_path,
            'ã‚¨ãƒ©ãƒ¼': str(e),
            'input_tokens': 0,
            'output_tokens': 0,
            'cached_input_tokens': 0,
            'total_tokens': 0
        }
        return result_data

def main():
    parser = argparse.ArgumentParser(description='Lambdaé–¢æ•°ã®çµæœã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒ')
    parser.add_argument('--limit', type=int, default=5, help='å‡¦ç†ã™ã‚‹ä»¶æ•°ã®ä¸Šé™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰')
    parser.add_argument('--csv', type=str, default='data/clipping_0521.csv', help='CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', type=str, default='comparison_results.csv', help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãªæ¯”è¼ƒçµæœã‚’è¡¨ç¤º')
    parser.add_argument('--workers', type=int, default=4, help='ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰')
    
    args = parser.parse_args()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.csv}")
    df = read_csv_data(args.csv)
    
    # å‡¦ç†ä»¶æ•°ã‚’åˆ¶é™
    df_limited = df.head(args.limit)
    print(f"å‡¦ç†å¯¾è±¡: {len(df_limited)}ä»¶")
    print(f"ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {args.workers}")
    
    results = []
    total_tokens = 0
    completed_count = 0
    
    # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºç”¨ã®ãƒ­ãƒƒã‚¯
    progress_lock = threading.Lock()
    
    def update_progress(result_data):
        nonlocal completed_count, total_tokens
        with progress_lock:
            completed_count += 1
            total_tokens += result_data.get('tokens', 0)
            
            status_emoji = "âœ…" if result_data['status'] == 'success' else "âŒ"
            print(f"{status_emoji} ({completed_count}/{len(df_limited)}) {result_data['filename']}")
            
            if result_data['status'] == 'success':
                match_rate = result_data.get('match_rate', 0)
                match_count = result_data.get('match_count', 0)
                total_fields = result_data.get('total_fields', 0)
                tokens = result_data.get('tokens', 0)
                print(f"   ä¸€è‡´ç‡: {match_rate:.1f}% ({match_count}/{total_fields}) | ãƒˆãƒ¼ã‚¯ãƒ³: {tokens:,}")
                
                # è©³ç´°è¡¨ç¤ºï¼ˆverboseãƒ¢ãƒ¼ãƒ‰ï¼‰
                if args.verbose and 'verbose_info' in result_data:
                    print(f"   è©³ç´°æ¯”è¼ƒçµæœ:")
                    for info in result_data['verbose_info']:
                        print(f"   {info}")
                        
                # è­¦å‘Šè¡¨ç¤º
                if 'warning' in result_data:
                    print(f"   âš ï¸ {result_data['warning']}")
            else:
                print(f"   ã‚¨ãƒ©ãƒ¼: {result_data['error']}")
    
    print(f"\nğŸš€ ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹...")
    start_time = time.time()
    
    # ä¸¦åˆ—å‡¦ç†ã§å„è¡Œã‚’å‡¦ç†
    with ProcessPoolExecutor(max_workers=args.workers) as executor:
        # ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
        future_to_index = {}
        for index, row in df_limited.iterrows():
            future = executor.submit(process_single_row, (index, row, args.verbose))
            future_to_index[future] = index
        
        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
        for future in as_completed(future_to_index):
            try:
                result_data = future.result()
                results.append(result_data['result_row'])
                update_progress(result_data)
                
            except Exception as e:
                index = future_to_index[future]
                row = df_limited.iloc[index - df_limited.index[0]]
                filename = row['ãƒ•ã‚¡ã‚¤ãƒ«å']
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {str(e)}")
                
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®çµæœã‚’è¿½åŠ 
                error_row = {
                    'ãƒ•ã‚¡ã‚¤ãƒ«å': filename,
                    'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹': row['ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'],
                    'ã‚¨ãƒ©ãƒ¼': f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'cached_input_tokens': 0,
                    'total_tokens': 0
                }
                results.append(error_row)
                completed_count += 1
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\nâ±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡å‡¦ç†æ™‚é–“: {processing_time/len(df_limited):.2f}ç§’/ä»¶")
    
    # çµæœã‚’DataFrameã«å¤‰æ›ã—ã¦ä¿å­˜
    if results:
        results_df = pd.DataFrame(results)
        results_df.to_csv(args.output, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“„ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {args.output}")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
        print(f"ç·ä»¶æ•°: {len(results)}")
        print(f"ç·ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {total_tokens:,}")
        
        if len(results) > 0:
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è©³ç´°çµ±è¨ˆ
            input_total = sum(row.get('input_tokens', 0) for row in results if isinstance(row.get('input_tokens'), int))
            output_total = sum(row.get('output_tokens', 0) for row in results if isinstance(row.get('output_tokens'), int))
            cached_total = sum(row.get('cached_input_tokens', 0) for row in results if isinstance(row.get('cached_input_tokens'), int))
            
            print(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆè¨ˆ: {input_total:,}")
            print(f"å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆè¨ˆ: {output_total:,}")
            if cached_total > 0:
                print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆè¨ˆ: {cached_total:,}")
            
            # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ä¸€è‡´ç‡ã‚’è¨ˆç®—ï¼ˆ_predåˆ—ã¨ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¯é™¤å¤–ï¼‰
            field_columns = [col for col in results_df.columns 
                           if col not in ['ãƒ•ã‚¡ã‚¤ãƒ«å', 'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹', 'input_tokens', 'output_tokens', 'cached_input_tokens', 'total_tokens', 'ã‚¨ãƒ©ãƒ¼'] 
                           and not col.endswith('_pred')]
            if field_columns:
                print(f"\nğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥ä¸€è‡´ç‡:")
                for field in field_columns:
                    if field in results_df.columns:
                        match_count = results_df[field].sum() if results_df[field].dtype == bool else 0
                        total_count = len(results_df[results_df[field].notna()])
                        if total_count > 0:
                            rate = (match_count / total_count) * 100
                            print(f"  {field}: {rate:.1f}% ({match_count}/{total_count})")
    else:
        print("âš ï¸ å‡¦ç†ã•ã‚ŒãŸçµæœãŒã‚ã‚Šã¾ã›ã‚“")

if __name__ == '__main__':
    main()
